---
title: "Relative Abundance Error Rate"
author: "Nate Olson"
date: '`r Sys.Date()`'
output:
  bookdown::pdf_document2: 
    toc: FALSE
---

```{r relAbuSetup, include=FALSE}
## TODO feature count table
library(nlme)
library(ape)
library(multcomp)
library(kableExtra)
library(tidyverse)
library(forcats)
library(ggpubr)
nb_counts <- readRDS("~/Desktop/nb_counts_titrations.RDS")
pa_summary_anno_df <- readRDS("~/Desktop/pa_summary_anno_df.RDS")
theta_est <- readRDS("~/Desktop/bootstrap_theta_estimates.rds")
```

```{r relAbuMunge, echo = FALSE, message = FALSE, warning = FALSE}
pre_post_prop <- nb_counts %>% 
      ungroup() %>% 
      filter(t_fctr %in% c(0,20)) %>% 
      mutate(end_point = if_else(t_fctr == 0 , "post", "pre")) %>% 
      dplyr::select(-t_fctr) %>% 
      ## setting values to 0 when one or more of the PCR replicates are 0 for titration end-points
      spread(end_point,nb_prop, fill = 0)

prop_inferred <- theta_est %>% 
      filter(pipe == "unclustered") %>% 
      ungroup() %>%
      mutate(t_fctr = factor(t_fctr, levels = c(0:5, 10, 15, 20))) %>% 
      dplyr::select(biosample_id, theta_hat_mean, t_fctr) %>% 
      right_join(nb_counts) %>% right_join(pre_post_prop) %>% 
      filter(t_fctr %in% c(1:5,10,15)) %>% 
      ## Using inferred theta estimates to calculate expected values
      mutate(inferred_prop = post * theta_hat_mean + pre * (1-theta_hat_mean))

## Excluding mix and unmix specific features
## Only including features observed in all or none of the four pre- post- PCR replicates
## Features with relative abundance estimates and expected values less than 1e-5, these are features that we would not expect to consistently observe in a PCR replicate for the given sequencing depth, ~100k 
## Excluding titrations where the inferred theta values are less than 1
pa_filter <- pa_summary_anno_df %>% 
      filter(pa_specific == "unspecific") %>% 
      dplyr::select(biosample_id, pipe, feature_id, full_pre, T00, T20, pa_mixed) %>% 
      filter(T00 %in% c(0,4), T20 %in% c(0,4))

prop_inferred <- prop_inferred %>% 
      right_join(pa_filter) %>% 
      filter(nb_prop > 1e-5, 
             inferred_prop > 1e-5,
             theta_hat_mean > 0)


#### Error Rate Calculations
rel_abu_error <- prop_inferred %>% 
      mutate(t_fctr = factor(t_fctr, levels = c(1:5, 10, 15))) %>% 
      mutate(inferred_error = nb_prop - inferred_prop,
             inferred_error_rate = inferred_error/inferred_prop) 

rel_abu_error_summary <-  rel_abu_error %>% 
      group_by(pipe, biosample_id, feature_id) %>% 
      summarise(median_error = median(inferred_error_rate),
                iqr_error = IQR(inferred_error_rate),
                rcov_error = iqr_error/median_error, 
                mean_error = mean(inferred_error_rate),
                var_error = var(inferred_error_rate),
                cov_error = var_error/mean_error) 

### Error rate boxplot and outlier annotation
error_boxplot <- rel_abu_error %>% group_by(pipe, biosample_id, feature_id) %>% 
      summarise(median_error = median(inferred_error_rate)) %>%
      ggplot() + 
      geom_boxplot(aes(x = pipe, y = median_error), outlier.shape = NA) + 
      facet_wrap(~biosample_id, nrow = 1) + 
      theme_bw() + theme(axis.text.x = element_text(angle = 90)) +
      labs(x = "Individual", y = "Median")

## Annotating features as outliers based on boxplot
error_plot_dat <- ggplot_build(error_boxplot)$data[[1]] %>% 
      mutate(pipe = fct_recode(factor(group), 
                               dada2 = "1", 
                               mothur = "2", 
                               qiime = "3",
                               unclustered = "4"),
             biosample_id = fct_recode(PANEL, 
                                       E01JH0004 = "1", 
                                       E01JH0011 = "2", 
                                       E01JH0016 = "3", 
                                       E01JH0017 = "4", 
                                       E01JH0038 = "5"))
outlier_error_dat <- error_plot_dat %>% 
      dplyr::select(ymin, ymax, pipe, biosample_id)

rel_error_outlier_cat <- rel_abu_error_summary %>% 
      left_join(outlier_error_dat) %>% 
      mutate(outlier_cat = if_else(median_error < ymin | median_error > ymax, "outlier","inlier")) 

## Robust COV Analysis
rcov_boxplot <- rel_abu_error_summary %>%
      ggplot() + geom_boxplot(aes(x = pipe, y = rcov_error), outlier.shape = NA) + 
      facet_wrap(~biosample_id, nrow = 1) + 
      theme_bw() + theme(axis.text.x = element_text(angle = 90)) +
      labs(x = "Individual", y = "RCOV") 


## Annotating features as outliers based on boxplot
rcov_plot_dat <- ggplot_build(rcov_boxplot)$data[[1]] %>% 
     mutate(pipe = fct_recode(factor(group), 
                               dada2 = "1", 
                               mothur = "2", 
                               qiime = "3",
                               unclustered = "4"),
             biosample_id = fct_recode(PANEL, 
                                       E01JH0004 = "1", 
                                       E01JH0011 = "2", 
                                       E01JH0016 = "3", 
                                       E01JH0017 = "4", 
                                       E01JH0038 = "5"))
outlier_rcov_dat <- rcov_plot_dat %>% 
      dplyr::select(ymin, ymax, pipe, biosample_id)

rcov_outlier_cat <- rel_abu_error_summary %>% 
      left_join(outlier_rcov_dat) %>% 
      mutate(outlier_cat = if_else(median_error < ymin | median_error > ymax, "outlier","inlier"))

### Plot Code -----------------------------------------------------------------
## Observed v. Expected Scatter plot
relAbuOvE <- prop_inferred %>% 
      ggplot() + 
      geom_point(aes(x = inferred_prop, y = nb_prop), alpha = 0.15) +
      geom_abline(aes(intercept = 0, slope = 1), color = "darkorange") +
      geom_smooth(aes(x = inferred_prop, y = nb_prop)) +
      facet_grid(pipe~biosample_id) +
      scale_y_log10() + scale_x_log10() +
      theme_bw() +
      labs(x = "Expected", 
           y = "Observed")

# ggsave(relAbuOvE, "~/Desktop/quant_exp_vs_obs.png", dpi = 450)

## Median Error Pipeline Comparison
ymin <- ggplot_build(error_boxplot)$data[[1]]$ymin %>% min()
ymax <- ggplot_build(error_boxplot)$data[[1]]$ymax %>% max()
error_boxplot <- error_boxplot + coord_cartesian(ylim = c(ymin, ymax))

# ggsave(error_boxplot, "~/Desktop/quant_bias.png", dpi = 450) 

## RCOV Error Pipeline Comparison
ymin <- ggplot_build(rcov_boxplot)$data[[1]]$ymin %>% min()
ymax <- ggplot_build(rcov_boxplot)$data[[1]]$ymax %>% max()
rcov_boxplot <- rcov_boxplot + coord_cartesian(ylim = c(ymin, ymax))

# ggsave(rcov_boxplot, "~/Desktop/quant_variance.png", dpi = 450)
```


```{r relAbuStats, echo = FALSE, message = FALSE, warning = FALSE}
## Bias - Error rate
error_fit_dat <- rel_error_outlier_cat %>% 
      ungroup() %>% 
      filter(outlier_cat == "inlier", pipe != "unclustered") %>% 
      mutate(pipe = factor(pipe))

# Fitting mixed effects model with individual as the fixed effect
error_fit <- nlme::lme(median_error ~ pipe, random =  ~ 1 | biosample_id, 
                 data = error_fit_dat)  

# Pipe error estimates
dada_error <- error_fit$coefficients$fixed['(Intercept)']
mothur_error <- dada_error + error_fit$coefficients$fixed['pipemothur']
qiime_error <- dada_error + error_fit$coefficients$fixed['pipeqiime']

# Post-hoc test to check for pipeline differences 
## based on fit pipeline estimates are all negative, using alternative greater to determine which pipelines are closer to zero
error_post_hoc <- glht(error_fit, linfct=mcp(pipe="Tukey"), alternative = "less") 

error_tukey_p <- summary(error_post_hoc)$test$pvalues 
error_tukey_t <- summary(error_post_hoc)$test$tstat

# Checking whether indiviudal or pipeline contributes more to the overal vaiance
error_var <- ape::varcomp(error_fit)

## Variance - RCOV
rcov_fit_dat <- rcov_outlier_cat %>% 
      ungroup() %>% 
      filter(outlier_cat == "inlier", pipe != "unclustered") %>% 
      mutate(pipe = factor(pipe))

# Fitting mixed effects model with individual as the fixed effect
rcov_fit <- nlme::lme(rcov_error ~ pipe, random =  ~ 1 | biosample_id, 
                 data = rcov_fit_dat)

# Pipe RCOV estimates
dada_rcov <- rcov_fit$coefficients$fixed['(Intercept)']
mothur_rcov <- dada_rcov + rcov_fit$coefficients$fixed['pipemothur']
qiime_rcov <- dada_rcov + rcov_fit$coefficients$fixed['pipeqiime']

# Checking whether indiviudal or pipeline contributes more to the overal vaiance
rcov_var <- ape::varcomp(rcov_fit)
```

```{r relAbuError, fig.cap = "(A) Expected and observed count relationship. Orange line indicates expected 1-to-1 relationship. Blue line a smoothed regression line of the observed and expected value relationship. Distribution of feature-level relative abundance (B) median error rates and (C) robust coefficient of variation (RCOV) by individual and pipeline.", echo = FALSE, warning=FALSE, message = FALSE, fig.height = 8}
ggarrange(relAbuOvE, 
          error_boxplot + rremove("x.text"), 
          rcov_boxplot, 
          labels = c("A", "B", "C"),
          align = "v", ncol = 1, nrow = 3, heights = c(0.45,0.2,0.35))
```

```{r relAbuErrorTbl, echo = FALSE, warning = FALSE, message = FALSE}
rel_abu_error_summary %>% 
      group_by(pipe, biosample_id) %>% 
      summarise(med_med_error = median(median_error), 
                min_med_error = min(median_error), 
                max_med_error = max(median_error),
                med_rcov_error = median(rcov_error), 
                min_rcov_error = min(rcov_error), 
                max_rcov_error = max(rcov_error)) %>% 
      ## Value ranges
      # mutate(Median = paste0(round(med_med_error,2), " (",
      #                        round(max_med_error,2), "-",
      #                        round(min_med_error,2),")"),
      #        RCOV = paste0(round(med_rcov_error,2), " (",
      #                      round(max_rcov_error,2), "-",
      #                      round(min_rcov_error,2),")")) %>% 
      # Max values only 
      mutate(Median = max_med_error, RCOV = max_rcov_error) %>% 
      select(pipe, biosample_id, Median, RCOV) %>%
      dplyr::rename(Pipeline = pipe, Individual = biosample_id) %>%
      gather("Metric","value", -Pipeline, -Individual) %>% 
      spread(Individual, value) %>% 
      arrange(Metric) %>% 
      select(Metric, Pipeline, E01JH0004, E01JH0011, E01JH0016, E01JH0017, E01JH0038) %>% 
      knitr::kable(booktabs = TRUE, caption = "Maximum feature-level error rate bias (median error rate) and variance (robust COV) by pipeline and individual.", digits = 2) %>% 
      collapse_rows(columns = 1)
```

__NOTE__  Not sure about interpretation of RCOV when using non-absolute feature-level error rate, how to interpret positive and negative values.  


Overall aggreement between the inferred and observed relative abundance was high for all individuals and bioinformatic pipelines (Fig. \@ref(fig:relAbuError)A). 
The pre- and post-exposure estimated relative abundance and inferred theta values were used to calculate titration and feature level error rates. 
Only features observed in all pre- and post-exposure PCR replicates and pre- and post-exposure specific features were included in the analysis (Table \@ref(tab:relAbuFeatTbl)). 
Pre- and post-exposure specific features were defined as present in all four PCR replicates of the pre-exposure or post-exposure PCR replicates, respectively, but none of the PCR replicates for the other unmixed sample.
There is lower confidence in the relative abundance of a feature in the pre- or post-exposure unmixed samples when the feature is observed in some of the 4 PCR replicates, therefore these features were not included in the error analysis.
The deviation from the expected value on the low end varies by biological replicate and pipeline. 
Outliers are observed for all pipelines and individuals.  

Next we evaluated the quantitative accuracy of the relative abundance values by comparing the distribution of the feature-level median error and feature-level robust coefficient (RCOV=(maximum - minimum)/median) of variation for the relative abundance error rate across pipelines (Fig. \@ref(fig:relAbuError)). 
Feature-level median error rates and RCOV were compared across pipelines and individuals using a mixed effects model. 
Large error rates and RCOV values were observed for all pipelines (Table  \@ref(tab:relAbuErrorTbl)). 
Features with large error rates, defined as $1.5\times IQR$ from the median, were excluded from the analysis to prevent outliers from biasing the comparison. 
The mean feature-level error rate by pipeline was negative for all three pipelines (DADA2 `r round(dada_error,2)`, Mothur `r round(mothur_error,2)`, and QIIME `r round(qiime_error,2)`). 
Multiple comparisons test (Tukey) was used to test for significant differences in feature-level error between pipelines. 
A one-sided alternative hypothesis to determine which pipelines had a smaller, closer to zero, feature-level error rate. 
The Mothur and DADA2 mean feature-level error rate were closer to zero and signficantly different from the QIIME pipeline, (Mothur v. QIIME t = `r  round(error_tukey_t['qiime - mothur'],2)`, p = `r round(error_tukey_p['qiime - mothur'],2)`; DADA2 v. QIIME t = `r  round(error_tukey_t['qiime - dada2'],2)`, p = `r round(error_tukey_p['qiime - dada2'],2)`). 
Though the Mothur and DADA2 mean feature-level error rates were not significantly different from eachother, t = `r  round(error_tukey_t['mothur - dada2'],2)`, p = `r round(error_tukey_p['mothur - dada2'],2)`.
Unlike feature-level error rates, large RCOV was observed for all individuals and pipelines (Table \@ref(tab:relAbuErrorTbl)). 
Outlier values were also excluded from the RCOV analysis. 
The feature-level RCOV was not significantly different between pipelines, Mothur = `r round(mothur_rcov,2)`, QIIME = `r round(qiime_rcov,2) ` and DADA2 = `r round(dada_rcov)` (Fig. \@ref(fig:relAbuError)C). 


